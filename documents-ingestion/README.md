# Document Ingestion Pipeline

Pipeline para carregar, quebrar em chunks semÃ¢nticos e indexar documentos jurÃ­dicos no Weaviate. Desenvolvido para ser usado por um agente de IA especialista em guidelines de comunicaÃ§Ãµes de CRM.

## ğŸ—ï¸ Arquitetura

O pipeline segue uma arquitetura modular e madura:

- **Extractors**: ExtraÃ§Ã£o de texto de PDFs (pymupdf)
- **Chunkers**: Chunking semÃ¢ntico baseado em estrutura de documento (tÃ­tulos, seÃ§Ãµes, listas)
- **Embeddings**: Suporte para OpenAI e modelos open-source (Ollama/local)
- **Indexers**: IndexaÃ§Ã£o no Weaviate com versionamento e idempotÃªncia
- **OrquestraÃ§Ã£o**: Pipeline Python idempotente e observÃ¡vel

## ğŸš€ ExecuÃ§Ã£o RÃ¡pida com Docker

### 1. Configure as variÃ¡veis de ambiente

```bash
cp .env.example .env
```

Edite o `.env` e configure principalmente:
- `OPENAI_API_KEY`: Sua chave da API OpenAI (se usando OpenAI)
- `EMBEDDING_PROVIDER`: `openai` ou `ollama`

### 2. Execute tudo com Docker Compose

```bash
docker-compose up
```

Isso irÃ¡:
1. Iniciar o Weaviate
2. Aguardar o Weaviate ficar pronto
3. Executar a pipeline de ingestÃ£o
4. Processar todos os PDFs em `doc-juridico/`

### 3. Executar apenas o Weaviate (para testes)

```bash
docker-compose up weaviate
```

### 4. Re-executar apenas a ingestÃ£o

```bash
docker-compose up ingestion
```

## ğŸ“‹ PrÃ©-requisitos

- Docker e Docker Compose
- API Key para embeddings (se usando OpenAI)

## ğŸ”§ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente (.env)

```bash
# Weaviate (nÃ£o precisa mudar se usando docker-compose)
WEAVIATE_URL=http://weaviate:8080

# Embeddings
EMBEDDING_PROVIDER=openai  # ou 'ollama'
OPENAI_API_KEY=sua_chave_aqui
EMBEDDING_MODEL=text-embedding-3-small  # opcional

# Documentos
DOCUMENTS_DIR=doc-juridico

# Logs
LOG_LEVEL=INFO
```

### Usando Ollama (local)

```bash
EMBEDDING_PROVIDER=ollama
OPENAI_BASE_URL=http://host.docker.internal:11434
EMBEDDING_MODEL=nomic-embed-text
```

## ğŸ“– Uso

### Executar Pipeline Completa

```bash
docker-compose up
```

### Executar Apenas Weaviate

```bash
docker-compose up -d weaviate
```

Depois execute a pipeline localmente:
```bash
python -m src.pipeline
```

### Usar como MÃ³dulo Python

```python
from pathlib import Path
from src.pipeline import IngestionPipeline

pipeline = IngestionPipeline(
    documents_dir=Path("doc-juridico"),
    embedding_provider="openai",
    weaviate_url="http://localhost:8080",
)

stats = pipeline.process_all()
pipeline.close()
```

## ğŸ³ Docker

### Construir imagem manualmente

```bash
docker build -t doc-ingestion .
```

### Executar container manualmente

```bash
docker run --rm \
  --env-file .env \
  -v $(pwd)/doc-juridico:/app/doc-juridico \
  --network host \
  doc-ingestion
```

## ğŸ”§ CaracterÃ­sticas Principais

### Chunking SemÃ¢ntico

O chunker nÃ£o divide por contagem fixa de tokens. Em vez disso, divide baseado em:

- TÃ­tulos e cabeÃ§alhos (detecÃ§Ã£o automÃ¡tica)
- SeÃ§Ãµes numeradas
- Quebras de seÃ§Ã£o (linhas separadoras, espaÃ§os)
- Listas e exemplos
- PreservaÃ§Ã£o de contexto estrutural

### Versionamento e IdempotÃªncia

- Cada documento recebe uma versÃ£o baseada em hash
- Chunks sÃ£o identificados deterministicamente
- Re-execuÃ§Ãµes sÃ£o idempotentes (nÃ£o duplicam dados)
- Suporte para rollback e auditoria

### Observabilidade

- Logs estruturados em JSON
- MÃ©tricas de processamento (chunks criados, indexados, erros)
- Rastreamento por `ingestion_run_id`

## ğŸ“ Estrutura do Projeto

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extractors/      # Extractores de documentos
â”‚   â”‚   â””â”€â”€ pdf_extractor.py
â”‚   â”œâ”€â”€ chunkers/        # Chunkers semÃ¢nticos
â”‚   â”‚   â””â”€â”€ semantic_chunker.py
â”‚   â”œâ”€â”€ embeddings/      # ServiÃ§os de embeddings
â”‚   â”‚   â””â”€â”€ embedding_service.py
â”‚   â”œâ”€â”€ indexers/        # Indexadores Weaviate
â”‚   â”‚   â””â”€â”€ weaviate_indexer.py
â”‚   â”œâ”€â”€ utils/           # UtilitÃ¡rios
â”‚   â”‚   â””â”€â”€ logging_config.py
â”‚   â””â”€â”€ pipeline.py      # Pipeline principal
â”œâ”€â”€ doc-juridico/        # Documentos a processar
â”œâ”€â”€ docker-compose.yml   # OrquestraÃ§Ã£o Docker
â”œâ”€â”€ Dockerfile           # Imagem da aplicaÃ§Ã£o
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ” PrÃ³ximos Passos

- [ ] Implementar retry e backoff para APIs externas
- [ ] Adicionar mÃ©tricas Prometheus/Grafana
- [ ] Suporte para pipelines event-driven
- [ ] AvaliaÃ§Ã£o de qualidade de chunks (offline eval)

## ğŸ“ Notas

Este pipeline segue prÃ¡ticas maduras de engenharia de dados para RAG:

- **IdempotÃªncia**: ExecuÃ§Ãµes repetidas nÃ£o duplicam dados
- **Versionamento**: HistÃ³rico de versÃµes de documentos
- **Observabilidade**: Logs estruturados e mÃ©tricas
- **Chunking Inteligente**: Baseado em estrutura, nÃ£o apenas tokens
- **Modularidade**: Componentes reutilizÃ¡veis e testÃ¡veis

Este setup Ã© mais maduro que 80% das POCs de RAG e alinhado com prÃ¡ticas de mercado.
