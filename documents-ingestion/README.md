# Document Ingestion Pipeline

Pipeline para carregar, quebrar em chunks semÃ¢nticos e indexar documentos jurÃ­dicos no Weaviate. Desenvolvido para ser usado por um agente de IA especialista em guidelines de comunicaÃ§Ãµes de CRM.

## âš ï¸ Nota Importante

**O Orqestra jÃ¡ vem com dados prÃ©-processados!**

O `docker-compose.yml` principal (na raiz do projeto) carrega automaticamente os chunks e vetores prÃ©-gerados a partir dos JSONs em `legal-service/data/`. Isso:

- âœ… Elimina necessidade de tokens OpenAI para ingestÃ£o
- âœ… Reduz tempo de startup (~segundos vs ~2-3 minutos)
- âœ… Garante reprodutibilidade para avaliadores

**Use este serviÃ§o apenas se quiser regenerar os dados** (ex: novos documentos, experimentos com chunking).

## ğŸ—ï¸ Arquitetura

O pipeline segue uma arquitetura modular e madura:

- **Extractors**: ExtraÃ§Ã£o de texto de PDFs (pymupdf)
- **Chunkers**: Chunking semÃ¢ntico baseado em estrutura de documento (tÃ­tulos, seÃ§Ãµes, listas)
- **Embeddings**: Suporte para OpenAI e modelos open-source (Ollama/local)
- **Indexers**: IndexaÃ§Ã£o no Weaviate com versionamento e idempotÃªncia
- **OrquestraÃ§Ã£o**: Pipeline Python idempotente e observÃ¡vel

## ğŸš€ ExecuÃ§Ã£o (RegeneraÃ§Ã£o de Dados)

### PrÃ©-requisitos

1. O Weaviate principal deve estar rodando:
   ```bash
   cd ..  # raiz do projeto
   docker compose up weaviate -d
   ```

2. Configure a variÃ¡vel de ambiente:
   ```bash
   export OPENAI_API_KEY=sua-chave-aqui
   ```

### Regenerar dados

```bash
# Dentro de documents-ingestion/

# Section chunking â†’ LegalDocuments
docker compose up ingestion-section

# Semantic chunking â†’ LegalDocumentsSemanticChunks
docker compose up ingestion-semantic

# Ou ambos sequencialmente
docker compose up ingestion-section && docker compose up ingestion-semantic
```

### Script helper

```bash
./scripts/ingest_all_strategies.sh
```

## ğŸ“‹ PrÃ©-requisitos

- Docker e Docker Compose
- API Key para embeddings (se usando OpenAI)

## ğŸ”§ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente (.env)

```bash
# Weaviate (nÃ£o precisa mudar se usando docker-compose)
WEAVIATE_URL=http://weaviate:8080

# Embeddings
EMBEDDING_PROVIDER=openai  # ou 'ollama'
OPENAI_API_KEY=sua_chave_aqui
EMBEDDING_MODEL=text-embedding-3-small  # opcional

# Documentos
DOCUMENTS_DIR=doc-juridico

# Logs
LOG_LEVEL=INFO
```

### Usando Ollama (local)

```bash
EMBEDDING_PROVIDER=ollama
OPENAI_BASE_URL=http://host.docker.internal:11434
EMBEDDING_MODEL=nomic-embed-text
```

## ğŸ“– Uso

### Executar Pipeline Completa

```bash
docker-compose up
```

### EstratÃ©gias de Chunking

O pipeline suporta duas estratÃ©gias de chunking, cada uma criando uma collection diferente no Weaviate:

| EstratÃ©gia | Collection | DescriÃ§Ã£o |
|------------|------------|-----------|
| `section` | `LegalDocuments` | Chunking por seÃ§Ãµes numeradas do documento |
| `semantic` | `LegalDocumentsSemanticChunks` | Chunking baseado em similaridade semÃ¢ntica |

#### IngestÃ£o com estratÃ©gia especÃ­fica

```bash
# Section chunking (padrÃ£o) â†’ LegalDocuments
docker compose up ingestion-section

# Semantic chunking â†’ LegalDocumentsSemanticChunks
docker compose up ingestion-semantic

# Ou via variÃ¡vel de ambiente
CHUNKER_TYPE=semantic docker compose up ingestion
```

#### IngestÃ£o de todas as estratÃ©gias (para experimentos)

```bash
# Indexa em ambas as collections
./scripts/ingest_all_strategies.sh

# Apenas section
./scripts/ingest_all_strategies.sh --section-only

# Apenas semantic
./scripts/ingest_all_strategies.sh --semantic-only
```

### Executar Apenas Weaviate

```bash
docker-compose up -d weaviate
```

Depois execute a pipeline localmente:
```bash
python -m src.pipeline
```

### Usar como MÃ³dulo Python

```python
from pathlib import Path
from src.pipeline import IngestionPipeline

pipeline = IngestionPipeline(
    documents_dir=Path("doc-juridico"),
    embedding_provider="openai",
    weaviate_url="http://localhost:8080",
)

stats = pipeline.process_all()
pipeline.close()
```

## ğŸ³ Docker

### Construir imagem manualmente

```bash
docker build -t doc-ingestion .
```

### Executar container manualmente

```bash
docker run --rm \
  --env-file .env \
  -v $(pwd)/doc-juridico:/app/doc-juridico \
  --network host \
  doc-ingestion
```

## ğŸ”§ CaracterÃ­sticas Principais

### EstratÃ©gias de Chunking

O pipeline oferece duas estratÃ©gias de chunking configurÃ¡veis:

#### Section Chunker
Divide documentos por seÃ§Ãµes numeradas (ex: "1. IntroduÃ§Ã£o", "2. Diretrizes"). 
- Preserva a estrutura original do documento
- Ideal para documentos com formataÃ§Ã£o consistente
- Collection: `LegalDocuments`

#### Semantic Chunker
Usa embeddings para identificar pontos de quebra semÃ¢ntica.
- Chunks mais coesos semanticamente
- Usa LangChain SemanticChunker
- Collection: `LegalDocumentsSemanticChunks`

Ambos detectam automaticamente o canal (SMS, EMAIL, PUSH, APP) a partir do nome do arquivo.

### Versionamento e IdempotÃªncia

- Cada documento recebe uma versÃ£o baseada em hash
- Chunks sÃ£o identificados deterministicamente
- Re-execuÃ§Ãµes sÃ£o idempotentes (nÃ£o duplicam dados)
- Suporte para rollback e auditoria

### Observabilidade

- Logs estruturados em JSON
- MÃ©tricas de processamento (chunks criados, indexados, erros)
- Rastreamento por `ingestion_run_id`

## ğŸ“ Estrutura do Projeto

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extractors/      # Extractores de documentos
â”‚   â”‚   â””â”€â”€ pdf_extractor.py
â”‚   â”œâ”€â”€ chunkers/        # Chunkers semÃ¢nticos
â”‚   â”‚   â””â”€â”€ semantic_chunker.py
â”‚   â”œâ”€â”€ embeddings/      # ServiÃ§os de embeddings
â”‚   â”‚   â””â”€â”€ embedding_service.py
â”‚   â”œâ”€â”€ indexers/        # Indexadores Weaviate
â”‚   â”‚   â””â”€â”€ weaviate_indexer.py
â”‚   â”œâ”€â”€ utils/           # UtilitÃ¡rios
â”‚   â”‚   â””â”€â”€ logging_config.py
â”‚   â””â”€â”€ pipeline.py      # Pipeline principal
â”œâ”€â”€ doc-juridico/        # Documentos a processar
â”œâ”€â”€ docker-compose.yml   # OrquestraÃ§Ã£o Docker
â”œâ”€â”€ Dockerfile           # Imagem da aplicaÃ§Ã£o
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ” PrÃ³ximos Passos

- [ ] Implementar retry e backoff para APIs externas
- [ ] Adicionar mÃ©tricas Prometheus/Grafana
- [ ] Suporte para pipelines event-driven
- [ ] AvaliaÃ§Ã£o de qualidade de chunks (offline eval)

## ğŸ“ Notas

Este pipeline segue prÃ¡ticas maduras de engenharia de dados para RAG:

- **IdempotÃªncia**: ExecuÃ§Ãµes repetidas nÃ£o duplicam dados
- **Versionamento**: HistÃ³rico de versÃµes de documentos
- **Observabilidade**: Logs estruturados e mÃ©tricas
- **Chunking Inteligente**: Baseado em estrutura, nÃ£o apenas tokens
- **Modularidade**: Componentes reutilizÃ¡veis e testÃ¡veis

Este setup Ã© mais maduro que 80% das POCs de RAG e alinhado com prÃ¡ticas de mercado.
