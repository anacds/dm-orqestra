# Avalia√ß√£o - Legal Service

Este diret√≥rio cont√©m scripts e datasets para avaliar a qualidade do `legal-service`.

## Arquivos

- `eval_retrieval_dataset.csv`: Dataset com exemplos de valida√ß√£o, incluindo `question`, `ground_truth` (arquivos esperados), e `expected_decision`.
- `evaluate_retrieval.py`: Script Python que avalia **apenas o retrieval** (busca de documentos), calculando precision e recall baseados nos **arquivos** retornados vs. ground truth.
- `evaluate_generation.py`: Script Python que avalia **a gera√ß√£o** (decision APROVADO/REPROVADO), comparando as decis√µes geradas pelo agente com as esperadas do dataset.

## Avalia√ß√£o de Retrieval

### Pr√©-requisitos

1. **Weaviate rodando**: O Weaviate precisa estar acess√≠vel (via Docker ou localmente).
2. **Vari√°veis de ambiente**: Configure `WEAVIATE_URL` e `OPENAI_API_KEY` (para embeddings).
3. **Depend√™ncias**: Instale as depend√™ncias do `legal-service` (o script usa os mesmos m√≥dulos).

### Execu√ß√£o local (fora do Docker)

```bash
cd legal-service

# Certifique-se de que o Weaviate est√° acess√≠vel em localhost:8080
# (ou ajuste WEAVIATE_URL no ambiente)

python3 evals/evaluate_retrieval.py \
  --dataset evals/eval_retrieval_dataset.csv \
  --weaviate-url http://localhost:8080 \
  --output evals/retrieval_results.json
```

### Execu√ß√£o via Docker

Se o Weaviate estiver rodando no Docker Compose, voc√™ pode executar o script dentro de um container tempor√°rio:

```bash
# Na raiz do projeto
docker-compose run --rm legal-service python3 /app/evals/evaluate_retrieval.py \
  --dataset /app/evals/eval_retrieval_dataset.csv \
  --weaviate-url http://weaviate:8080 \
  --output /app/evals/retrieval_results.json
```

Ou, se preferir executar localmente mas apontando para o Weaviate no Docker:

```bash
cd legal-service
WEAVIATE_URL=http://localhost:8080 python3 evals/evaluate_retrieval.py
```

## M√©tricas calculadas

O script calcula **precision e recall por arquivo**:

- **Precision**: Dos arquivos retornados pelo retriever, quantos est√£o no ground truth?
  - `precision = |retrieved_files ‚à© ground_truth_files| / |retrieved_files|`
  
- **Recall**: Dos arquivos esperados (ground truth), quantos foram retornados?
  - `recall = |retrieved_files ‚à© ground_truth_files| / |ground_truth_files|`

- **F1**: M√©dia harm√¥nica de precision e recall.

### M√©tricas agregadas

- **Macro-average**: M√©dia aritm√©tica das m√©tricas de todos os exemplos.
- O script tamb√©m mostra os **top 5 piores** e **top 5 melhores** casos.

## Formato do dataset

O CSV deve ter as seguintes colunas (separadas por `;`):

- `channel`: Canal de comunica√ß√£o (SMS, PUSH, etc.)
- `content`: Conte√∫do da mensagem a ser validada
- `question`: Query completa para o retriever (formato: `VALIDATE_COMMUNICATION para CHANNEL: content`)
- `ground_truth`: Arquivos esperados, separados por `|` (ex: `file1.pdf|file2.pdf`)
- `expected_decision`: Decis√£o esperada (APROVADO/REPROVADO/etc.) - n√£o usado na avalia√ß√£o de retrieval

## Exemplo de sa√≠da

```
================================================================================
RESUMO DA AVALIA√á√ÉO DE RETRIEVAL
================================================================================

Total de exemplos: 40
Exemplos processados com sucesso: 40

M√©tricas agregadas (macro-average):
  Precision: 0.8234
  Recall:    0.7567
  F1:        0.7889

================================================================================

Top 5 piores casos (menor F1):
  [12] F1=0.000 | P=0.000 R=0.000
      Question: VALIDATE_COMMUNICATION para SMS: ORQESTRA: √öLTIMA CHANCE!!!...
      Expected: ['orqestra-guidelines-comunicacao-SMS_v1.pdf', ...]
      Retrieved: ['outro-arquivo.pdf']

...
```

## Interpreta√ß√£o dos resultados

- **Precision alta, Recall baixo**: O retriever est√° retornando arquivos relevantes, mas est√° perdendo alguns arquivos importantes.
  - **A√ß√£o**: Aumentar `limit` no `config/models.yaml` ou ajustar `alpha` (peso do hybrid search).

- **Recall alto, Precision baixo**: O retriever est√° retornando muitos arquivos, mas muitos s√£o irrelevantes.
  - **A√ß√£o**: Reduzir `limit` ou melhorar a qualidade dos embeddings/indexa√ß√£o.

- **Ambos baixos**: Problema mais grave na indexa√ß√£o ou na query.
  - **A√ß√£o**: Revisar como os documentos foram indexados, verificar se os metadados (`file_name`, `source_file`) est√£o corretos.

## Notas

- O script usa o mesmo `HybridWeaviateRetriever` e configura√ß√µes (`limit`, `alpha`) do servi√ßo em produ√ß√£o.
- A avalia√ß√£o √© **file-level**, n√£o text-level. Ou seja, medimos se os **arquivos certos** foram retornados, n√£o se o conte√∫do espec√≠fico dentro deles est√° correto.

---

## Avalia√ß√£o de Gera√ß√£o

### Pr√©-requisitos

1. **Weaviate rodando**: O Weaviate precisa estar acess√≠vel.
2. **Vari√°veis de ambiente**: Configure `WEAVIATE_URL`, `OPENAI_API_KEY` (para embeddings), e `MARITACA_API_KEY` ou `OPENAI_API_KEY` (para o LLM).
3. **Depend√™ncias**: Instale as depend√™ncias do `legal-service`.

### Execu√ß√£o local

```bash
cd legal-service

# Certifique-se de que o Weaviate e Redis est√£o acess√≠veis
export WEAVIATE_URL=http://localhost:8080
export OPENAI_API_KEY=sua_key_aqui
export MARITACA_API_KEY=sua_key_aqui  # Opcional, usa OpenAI como fallback

python3 evals/evaluate_generation.py \
  --dataset evals/eval_retrieval_dataset.csv \
  --weaviate-url http://localhost:8080 \
  --output evals/generation_results.json
```

### Execu√ß√£o via Docker

```bash
# Na raiz do projeto
docker-compose run --rm legal-service python3 /app/evals/evaluate_generation.py \
  --dataset /app/evals/eval_retrieval_dataset.csv \
  --weaviate-url http://weaviate:8080 \
  --output /app/evals/generation_results.json
```

### M√©tricas calculadas

O script avalia a qualidade das **decis√µes geradas** pelo agente completo (retrieval + LLM):

- **Accuracy (Decision)**: Percentual de exemplos onde `decision` (APROVADO/REPROVADO) est√° correta.
- **Precision/Recall/F1 por classe**: M√©tricas detalhadas para cada valor de `decision`.
- **Matriz de confus√£o**: Mostra onde o modelo est√° errando (ex.: classificando como APROVADO quando deveria ser REPROVADO).

### Mapeamento de expected_decision

O CSV tem valores como `APROVADO`, `BLOCKER`, `WARNING`. O script mapeia assim:

- `APROVADO` -> `decision=APROVADO`
- `BLOCKER` ou `WARNING` -> `decision=REPROVADO`

### Exemplo de sa√≠da

```
================================================================================
RESUMO DA AVALIA√á√ÉO DE GERA√á√ÉO
================================================================================

Configura√ß√£o:
  LLM: sabiazinho-4 (maritaca)
  Embeddings: text-embedding-3-small
  Cache: disabled

Total de exemplos: 40

Accuracy (Decision): 0.9250 (92.50%)

M√©tricas por Decision:
  APROVADO     | Precision: 0.9000 | Recall: 0.8571 | F1: 0.8780 | Support: 7
  REPROVADO    | Precision: 0.9375 | Recall: 0.9697 | F1: 0.9533 | Support: 33

...
```

### Notas

- Por padr√£o, o **cache est√° desabilitado** para garantir uma avalia√ß√£o justa (sem resultados de execu√ß√µes anteriores).
- O script usa o mesmo `LegalAgent` e configura√ß√µes do servi√ßo em produ√ß√£o (mesmo LLM: `sabiazinho-4` ou `gpt-5-nano` fallback).
- A avalia√ß√£o √© **simples**: compara apenas se a decis√£o est√° correta, n√£o avalia a qualidade do `summary` (justificativa).

---

## Sistema de Experimentos

O sistema de experimentos permite comparar diferentes configura√ß√µes do agente de forma sistem√°tica.

### Arquivos

- `experiments.yaml`: Arquivo de configura√ß√£o que define todos os experimentos dispon√≠veis.
- `run_experiments.py`: Script para executar experimentos com base no YAML.
- `results/`: Diret√≥rio onde os resultados s√£o salvos.

### Configura√ß√£o de Experimentos

O arquivo `experiments.yaml` define tr√™s dimens√µes de varia√ß√£o:

1. **Retrieval Strategy** (`retrieval_strategies`):
   - `hybrid`: Busca h√≠brida (BM25 + Vector, alpha=0.5) - **padr√£o**
   - `bm25`: Apenas busca por keywords (alpha=0.0)
   - `semantic`: Apenas busca vetorial (alpha=1.0)

2. **Chunking Strategy** (`chunking_collections`):
   - `section`: Chunking por se√ß√£o do documento - **padr√£o** (collection: `LegalDocuments`)
   - `semantic`: Chunking sem√¢ntico (collection: `LegalDocumentsSemanticChunks`)

3. **LLM Model** (`llm_models`):
   - `maritaca`: Maritaca sabiazinho-4 - **padr√£o**
   - `gpt_nano`: OpenAI GPT-5-nano

### Execu√ß√£o de Experimentos

```bash
# Listar experimentos dispon√≠veis
docker compose exec legal-service python evals/run_experiments.py --list

# Executar todos os experimentos habilitados
docker compose exec legal-service python evals/run_experiments.py

# Executar um experimento espec√≠fico
docker compose exec legal-service python evals/run_experiments.py --experiment baseline

# Usar arquivo de configura√ß√£o customizado
docker compose exec legal-service python evals/run_experiments.py --config evals/my_experiments.yaml
```

### Experimentos Pr√©-configurados

| ID | Nome | Retrieval | Chunking | LLM | Habilitado |
|----|------|-----------|----------|-----|------------|
| `baseline` | Baseline (produ√ß√£o) | hybrid | section | maritaca | ‚úì |
| `bm25_only` | BM25 Only | bm25 | section | maritaca | ‚úì |
| `semantic_only` | Semantic Only | semantic | section | maritaca | ‚úì |
| `semantic_chunking` | Semantic Chunking | hybrid | semantic | maritaca | ‚úó* |
| `gpt_nano` | GPT-5-nano | hybrid | section | gpt_nano | ‚úì |
| `bm25_gpt_nano` | BM25 + GPT-nano | bm25 | section | gpt_nano | ‚úì |
| `semantic_gpt_nano` | Semantic + GPT-nano | semantic | section | gpt_nano | ‚úì |

*Requer collection com chunking sem√¢ntico pr√©-indexada.

### Par√¢metros via Linha de Comando

O script `evaluate_generation.py` tamb√©m aceita par√¢metros de experimento diretamente:

```bash
# Testar com alpha diferente (0.0=BM25, 0.5=hybrid, 1.0=semantic)
docker compose exec legal-service python evals/evaluate_generation.py --alpha 0.0

# Testar com modelo espec√≠fico
docker compose exec legal-service python evals/evaluate_generation.py --model gpt-5-nano

# Combinar par√¢metros
docker compose exec legal-service python evals/evaluate_generation.py \
  --alpha 1.0 \
  --model sabiazinho-4 \
  --output evals/results/semantic_maritaca.json
```

### Relat√≥rio Comparativo

Ao executar m√∫ltiplos experimentos, um relat√≥rio comparativo √© gerado automaticamente:

```
================================================================================
RELAT√ìRIO COMPARATIVO DE EXPERIMENTOS
================================================================================

Experimento                    Retrieval    Alpha   LLM             Accuracy  
----------------------------------------------------------------------------------------------------
Baseline (produ√ß√£o)            hybrid       0.5     maritaca        87.50%
GPT-5-nano                     hybrid       0.5     gpt_nano        85.00%
BM25 Only                      bm25         0.0     maritaca        82.50%
Semantic Only                  semantic     1.0     maritaca        80.00%
================================================================================

üèÜ MELHOR RESULTADO: Baseline (produ√ß√£o) (87.50%)
```

Os resultados s√£o salvos em `evals/results/` nos formatos JSON e CSV.

### Criando Novos Experimentos

Para adicionar um novo experimento, edite `experiments.yaml`:

```yaml
experiments:
  meu_experimento:
    name: "Meu Experimento"
    description: "Descri√ß√£o do experimento"
    enabled: true
    retrieval_strategy: "hybrid"  # hybrid, bm25, ou semantic
    chunking: "section"           # section ou semantic
    llm_model: "maritaca"         # maritaca ou gpt_nano
```

### Notas sobre Experimentos

- **Chunking Sem√¢ntico**: Requer que a collection `LegalDocumentsSemanticChunks` exista no Weaviate com os documentos indexados usando chunking sem√¢ntico.
- **Tempo de Execu√ß√£o**: Cada experimento leva aproximadamente o mesmo tempo que uma avalia√ß√£o completa (~40 exemplos √ó ~1s/exemplo = ~40s).
- **Custo de API**: Cada experimento consome tokens de API (embeddings + LLM). Considere isso ao executar m√∫ltiplos experimentos.
